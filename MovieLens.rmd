---
title: "**MovieLens**"
author: "**Omkar Oka**"
date: "**6/15/2020**"
output:
  pdf_document: 
    fig_width: 10
    fig_height: 10
    toc: yes
    number_sections: yes
    latex_engine: xelatex
    keep_tex: yes
    highlight: tango
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\newpage

# **Executive summary:**  
Recommendation is the act of generating a suitable suggestion based on a user's shopping pattern and their reviews for certain products , music or movies either on a shopping or a streaming platform. Making the right recommendation for the next product, music or movie increases user retention and satisfaction, leading to sales and profit growth.

The most famous project that tried to achieve that goal was the **The Netflix Prize** (October 2006). This project was an open competition to predict user ratings for films, based on previous ratings without any other information about the users or films. The goal was to make the company's recommendation engine 10% more accurate.

In this project we try to generate a model that can predict the rating that a user will give to a movie based on their preference and previous interactions, in terms of genres and the overall effects of the movie. We will be using the MovieLens data set pre-processed by the GroupLens research lab in the University of Minnesota.


Further we will conduct an exploratory analysis of the data and based on some characteristics of the data we can decide the features that will be used to create a Machine Learning algorithm. This document explores step by step approach, process, techniques and methods that were used to handle the data and to create the predictive model.  

The final section shows the results of the previous process and then, the conclusion of the project with a future road map. Our goal is to achieve an RMSE of less than 0.86490.  

## **Evaluation Model:**    

The evaluation of machine learning algorithm consists comparing the predicted value with the actual outcome. The loss function measures the difference between both values.
We will be using the Root Mean Squared Error method for evaluating the performance of our model.

The Root Mean Squared Error, RMSE, is the square root of the MSE. It is the typical metric to evaluate recommendation systems, and is defined by the formula:

$$RMSE=\sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^2}$$

where N is the number of ratings, $y_{u,i}$ is the rating of movie i by user u and $\hat{y}_{u,i}$ is the prediction of movie i by user u.

RMSE penalizes large deviations from the mean and is appropriate in cases that small errors are not relevant. Contrary to other methods, the error has the same unit as the measurement.

## **Data Preparation:**    

In this section we download and prepare the dataset to be used in the analysis. We split the data set in two parts, the training set called edx and the evaluation set called validation with 90% and 10% of the original dataset respectively.

Then, we split the edx set in two parts, the train set and test set with 90% and 10% of edx set respectively. The model is created and trained in the train set and tested in the test set until the RMSE target is achieved, then finally we train the model again in the entire edx set and validate in the validation set. The name of this method is cross-validation.

\newpage
Extracting data publicly available from GroupLens:
```{r download, message = FALSE, warning = FALSE}
 #Create test and validation sets
 #Create edx set, validation set, and submission file
options(warn=-1) #Suppress Warnings
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(anytime)) install.packages("anytime", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(corrr)) install.packages("corrr", repos = "http://cran.us.r-project.org")
#Loading Libraries:
library("stringr")
library("tidyverse")
library("caret")
library("anytime")
library("lubridate")
library("corrr")

# MovieLens 10M dataset:
 # https://grouplens.org/datasets/movielens/10m/
 # http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
 download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
 colnames(movies) <- c("movieId", "title", "genres")
										
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId), title = as.character(title), genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
 
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% semi_join(edx, by = "movieId") %>% semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
\newpage
## **DataSet:**  

```{r structure, warning = FALSE, message = FALSE}
str(edx)
```
  
  
The data set contains 9000055 observations of 6 variables.  

```{r data, warning = FALSE, message = FALSE}
head(edx)
```
  
The Data Description is as follows:  
- `userId`: Unique identification number given to each user. `numeric` variable  
- `movieId`: Unique identification number given to each movie. `numeric` variable.  
- `timestamp`: Code that contains date and time in what the rating was given by the user to the specific movie. `integer` variable.  
- `title`: Title of the movie. `character` variable.  
- `genres`: Motion-picture category associated to the film. `character` variable.  
- `rating`: Rating given by the user to the movie. From 0 to 5 *stars* in steps of 0.5. `numeric` variable.   
\newpage

# **Analysis Section:**  
We will be performing necessary transformations, data preparation and exploratory data analysis as part of this section.  

## **Data format:**
The `userId` and `movieId` variables are `numeric` columns in the original data set. However it does not make sense. The `userId`=2 is not two times the `userId`=1 and the same effect happens with the `movieId` variable. These characteristics are just *labels*, therefore they will be converted to `factor` type to be useful.

Both `movieId` and `title` variables give us the same exact information. They are the **unique identification code** to each film. We could say that these pair of variables have 100% correlation! Only the `movieId` column will be used and will be used as a `factor`. It optimizes the memory (RAM) usage.

The `timestamp` variable is converted to `POSIXct` type, to be handle correctly as a `date` vector. The year is extracted to the `year` column and the `timestamp` column is dropped.

```{r formats, message = FALSE, echo = FALSE}
edx$userId <- as.factor(edx$userId) # Convert `userId` to `factor`.
edx$movieId <- as.factor(edx$movieId) # Convert `movieId` to `factor`.
edx$genres <- as.factor(edx$genres) # Convert `genres` to `factor`.
edx$timestamp <- as.POSIXct(edx$timestamp, origin = "1970-01-01") # Convert `timestamp to `POSIXct`.

edx <- edx %>%
  mutate(title = str_trim(title)) %>%
  extract(title, c("title_tmp", "year"),
          regex = "^(.*) \\(([0-9 \\-]*)\\)$",
          remove = F) %>%
  mutate(year = if_else(str_length(year) > 4,
                        as.integer(str_split(year, "-",
                                             simplify = T)[1]),
                        as.integer(year))) %>%
  mutate(title = if_else(is.na(title_tmp), title, title_tmp)) %>%
  select(-title_tmp) %>% mutate(year_rate = year(timestamp))

edx$genres <- as.factor(edx$genres)


head(edx)
```  
  
  
## **Exploratory Analysis:**  

The target is to create a model capable of predicting the variable `rating`. 
```{r rating_summary, message = FALSE, echo = FALSE}
summary(edx$rating)
```
  
As we can see from the distribution of the ratings the minimum rating is 0.5 whereas the first Quantile starts at 3.0 and the mean is 3.5 with the third Quantile at 4.0 which means that most of our ratings are in between 3 and 4 up to 4.5  
  

Users have the option to choose a rating value from 0.5 to 5.0, totaling 10 possible values. This is an unusual scale, so all the movies get a rounded value rating, as shown in the chart below.  
  
We can convert these ratings into factors and design the Machine Learning Algorithm as a classification model. But, since we have a target RMSE that we need to beat, its better to create a regression model since RMSE is not usually performed on a classification model.

  
  
\newpage

**Overall Distribution of ratings:**  
```{r Rating_Distribution, warning = FALSE, message = FALSE, echo = FALSE}
edx %>% group_by(rating) %>% summarise(n = n()) %>% 
        arrange(desc(n)) %>% 
        mutate(Percentage = paste(round(n / sum(n) * 100, 2), "%", sep = "")) %>%
        knitr::kable(caption = "Rating_Distribution")
```


```{r plot_rating, warning = FALSE, message = FALSE, echo = FALSE, fig.height = 7, fig.width = 10, fig.align = "center"}
edx %>% ggplot(aes(rating)) +
            geom_histogram(fill = "darkblue") +
            labs(title = "Rating distribution",
                 x = "Rate",
                 y = "Frequency")
```


We can see that our `rating` variable has a left-skewed distribution. It's interesting that there are more *good* ratings than *bad* ratings. That could be explained by the fact that people want to recommend a film when they like it, but we could just assume that, since there is no data available with this data set to prove this theory.  
\newpage

**Important Data Players:**  
```{r unique_users, warning = FALSE, message = FALSE, echo = FALSE}
edx %>% summarize(Unique_Users = n_distinct(userId), Unique_Movies = n_distinct(movieId),Unique_Genres = n_distinct(genres)) %>% knitr::kable(caption = "Unique_Numbers")

```

We observe that there are 69878 unique users given ratings to 10677 different films. It's good to remember that the *unique genres* were counted as `factor` with no previous separation so, `Drama` and `Comedy|Drama` are counted as 2 different genres.  
  
  
**Rates by Release Year:**  
```{r year_release, message = FALSE, warning = FALSE, echo = FALSE}
edx %>% ggplot(aes(year)) +
  geom_histogram(fill = "darkblue") +
  labs(title = "Release Year distribution",
       x = "Year",
       y = "Frequency")
```

The frequency of ratings by *release year* of the films has a clear left skewed distribution. The most of those year are between 1990 and 2009. Primary reason for this could be that since there is a clear age group that actively assign ratings and their preference is usually influenced by the era of the movies.  
  
  
**Average Movie Rating Distribution:**  
```{r average_rating, message = FALSE, warning = FALSE, echo = FALSE}
edx %>% group_by(movieId) %>% summarise(mean = mean(rating), sd = sd(rating), n = n()) %>% 
    ggplot(aes(mean)) + geom_histogram(fill = "darkblue") +
    labs(x = "Rating",
       y = "Frequency")
```
  
This is also a slightly left skewed distribution, since the average rating for most of the movies ranges from 2.5 to 4.0 stars.  
\newpage
**Genres and Ratings:**  
```{r Genres_vs_Ratings, message = FALSE, warning = FALSE, echo = FALSE}
genre_dist <- edx %>%
  select(genres, rating) %>% 
  group_by(genres) %>%
  summarize(mean = mean(rating), median = median(rating), n = n()) %>% arrange(desc(mean))

genre_dist %>% mutate(Percentage = paste(round(n / sum(n) * 100, 2), "%", sep = "")) %>% head(15) %>% knitr::kable(caption = "Genres_vs_Ratings")
```

The top 10 **genres** listed above have the highest **mean**. The first place "Animation|IMAX|Sci-Fi" is clearly not significant, because of the only 7 observations. This *genre* will be eliminated below. **Drama** is present in the 2nd and 3rd place.  

```{r appearences_top_20, message = FALSE, warning = FALSE, echo = FALSE}
genre_dist[2:nrow(genre_dist), ] %>%
  separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  head(15) %>% knitr::kable(caption = "Top_15_Genres")
```

The genres associated with the highest mean are "Drama", "Film-Noir" and "Romance". The **difference** with the *second section* entry numbers from 4 till 7 in that ranking is almost zero.  
\newpage
**Ratings over the Years:**  
```{r time_effect_ratings, message = FALSE, warning = FALSE, echo = FALSE}
edx %>% filter(movieId %in% c(2571,296,356,593,480,318,110,457,589,260,150,592,1)) %>% ggplot(aes(timestamp, rating, color = title)) + geom_smooth() + theme(legend.position = "bottom")
```
  
Here are a few of the movies with good overall average ratings rated by a significant number of users over the years. There seems to be a definite time effect on the ratings which could be related to the Actors or the similarity of the movies with the current ones playing in a  theater at the time. The time effect, actor influence or any other effect are out of the scope of this document but it still shows that there is a premise to explore further on these aspects.  
\newpage
## **Correlation Matrix:**  
```{r Feature_VS_Rating_Correlation,message = FALSE, warning = FALSE, echo = FALSE}
library('corrr')
x <- data.frame(userId = as.numeric(edx$userId), movieId = as.numeric(edx$movieId), rating = edx$rating, genres = as.numeric(edx$genres))
x <- as.matrix(x)
correlate(x) %>% knitr::kable(caption = "Feature_VS_Rating_Correlation")

```
  
Most of the Machine Learning algorithms prefer a comparable correlation within the features and between the features and the target(rating in this case). Since, as we can see the `UserId`, `MovieId` and `Genres` have a weak correlation with `Rating` any kind of supervised Machine Learning algorithm will not have much effect on the rating predictions and therefore will not be able to provide useful recommendations.  

Hence we will be sticking to a simple form of Linear Regression for Predicting ratings, which is covered in the next section.  
\newpage

# **The Model:**  

Creating a recommendation system involves the identification of the most important features that helps to predict the rating any given user will give to any movie. We start building a very simple model, which is just the mean of the observed values. Then, the user and movie effects are included in the linear model, improving the RMSE. Finally, the user and movie effects receive regularization parameter that penalizes samples with few ratings.

## **Train and Test set:**

First of all, we create `train` and `test` sets.

```{r create_train_test, message = FALSE, warning = FALSE}
edx <- edx %>% select(userId, movieId, rating)
set.seed(1, sample.kind="Rounding")
#set.seed(1) #for R version 3.5 and below
test_index <- createDataPartition(edx$rating, times = 1, p = .1, list = FALSE)
# Create the index

train <- edx[-test_index, ] # Create Train set
temp <- edx[test_index, ] # Create Test set

# Make sure userId and movieId in test set are also in train set
test <- temp %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

# Add rows removed from test set back into train set
removed <- anti_join(temp, test)
train <- rbind(train, removed)

rm(test_index, temp, removed)
```


```{r dim_sets, warning = FALSE}
dim(train)
dim(test)
```

## **Baseline model:**  
The most basic model is generated when we consider the most common rating from the *train* set to be predicted into the test set. This is the **baseline** model.  

```{r baseline, message = FALSE, warning = FALSE, echo = FALSE}
mu_hat <- mean(train$rating) # Mean across all movies.
RMSE_baseline <- RMSE(test$rating, mu_hat) # RMSE in test set.
RMSE_baseline
```  

Now, we have the baseline RMSE to be *beaten* by our model.  

```{r table_baseline, message = FALSE, warning = FALSE, echo = FALSE}
rmse_table <- data_frame(Method = "Baseline", RMSE = RMSE_baseline)
rmse_table %>% knitr::kable(caption = "RMSEs")
```  

We can observe that the RMSE of the most basic model is `r RMSE_baseline`. It's bigger than 1! In this context, this is a very bad model.  
  

## **User and Movie effect Model:**  
The next step is to improve our model and get a better RMSE by introducing *user effect*  $(u_i)$ and the *movie effect* $(m_i)$ as predictors. As seen in our exploratory analysis above, there is a definite impact of *user effect* with certain users assigning higher than usual or lower than usual ratings based on the their preference which usually varies from `0.5` to `0.7` in either direction.  
    *Movie effect* implies that there are certain movies that generally get rated higher than average based on their popularity.
The movie effect can be calculated as the mean of the difference between the observed rating y and the mean $\mu$.  

**Movie Effect:**
$$m_i=\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{\mu})$$
**User Effect:**
$$u_i=\frac{1}{N}\sum_{i=1}^{N}(y_{u,i}-\hat{b}_i-\hat{\mu})$$

Therefore, we are generating the next model to predict `rating` $(\hat{y}_i)$:
$$\hat{y}_i=u_i+m_i+\varepsilon$$
While calculating ratings as continuous values we can assume that there will be some predicted ratings that might be less than zero or greater than 5. We will be applying a balancing condition to counteract these values. We can call this the error$(\varepsilon)$  effect and it has been adjusted in the code already.  

**RMSE obtained by calculating the *user effect* and *movie effect*:**  
```{r normal_model, message = FALSE, warning = FALSE, echo = FALSE}
mu <- mean(train$rating)
movie_avgs <- train %>%
  group_by(movieId) %>%
  summarize(m_i = mean(rating - mu))
user_avgs <- test %>%
  left_join(movie_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarize(u_i = mean(rating - mu - m_i))
predicted_ratings <- test %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(pred = mu + m_i + u_i) %>% 
  mutate(pred = ifelse(pred < 0, 0, ifelse(pred > 5, 5, pred))) %>% .$pred
#The mutate condition is used to counter values less than 0 and greater than 5.
model_RMSE <- RMSE(predicted_ratings, test$rating)
model_RMSE  
```  

```{r table_usr_mov, message = FALSE, warning = FALSE, echo = FALSE}
rmse_table <- rbind(rmse_table,
                    data_frame(Method = "User & Movie Effect", RMSE = model_RMSE))
rmse_table %>% knitr::kable(caption = "RMSE")
```  

We've obtained a better RMSE. Now it is time to test the predictions on Validation data.  
\newpage

## **User and Movie effect Model on *validation* data:**  
First of all, the *validation* data set needs to be handled the same was as the *train* data set was handled.  

```{r val_data_set, message = FALSE, warning = FALSE}
validation <- validation %>% select(userId, movieId, rating)
validation$userId <- as.factor(validation$userId)
validation$movieId <- as.factor(validation$movieId)
validation <- validation[complete.cases(validation), ]
```

Now, we are ready to predict on the validation data set.  

```{r val_user_mov, message = FALSE, warning = FALSE}
mu <- mean(edx$rating)
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(m_i = mean(rating - mu))
user_avgs <- edx %>%
  left_join(movie_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarize(u_i = mean(rating - mu - m_i))
predicted_ratings <- validation %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(pred = mu + m_i + u_i) %>% mutate(pred = ifelse(pred < 0, 0, ifelse(pred > 5, 5, pred))) %>% .$pred

val_RMSE <- RMSE(predicted_ratings, validation$rating, na.rm = T)
```  
```{r table3_val, message = FALSE, warning = FALSE, echo = FALSE}
rmse_table <- rbind(rmse_table,
                    data_frame(Method = "User & Movie Effect on validation", RMSE = val_RMSE))
rmse_table %>% knitr::kable(caption = "RMSE")
```  

We can see above that this RMSE is higher than the RMSE on the test set. This is highly probable, given that this was unseen data. The good thing is that the difference is just `r val_RMSE - model_RMSE`. Now, let's see if *regularisation* will give us better results.
\newpage

## **Regularisation:**  

The linear model provides a good estimation for the ratings, but doesn’t consider that many movies have very few number of ratings, and some users rate very few movies. This means that the sample size is very small for these movies and these users. Statistically, this leads to large estimated error.  

The estimated value can be improved adding a factor that penalizes small sample sizes and have have little or no impact otherwise. Thus, estimated movie and user effects can be calculated with these formulas:  

**Movie Effect on Regularisation:**
$$m_i=\frac{1}{N_i+\lambda}\sum_{i=1}^{N_i}(y_{u,i}-\hat{\mu})$$
**User Effect on Regularisation:**
$$u_i=\frac{1}{N_u+\lambda}\sum_{i=1}^{N_i}(y_{u,i}-m_i-\hat{\mu})$$
For values of N smaller than or similar to $\lambda$, $m_i$ and $u_i$ is smaller than the original values, whereas for values of N much larger than $\lambda$, $m_i$ and $u_i$ change very little.  

The regularisation process will evaluate different values for $\lambda$, delivering to us the corresponding RMSE.  

```{r user_mov_reg, message = FALSE, warning = FALSE, echo = FALSE}
lambda_values <- seq(0, 7, .1)

RMSE_function_reg <- sapply(lambda_values, function(l){
  
  mu <- mean(train$rating)
  
  m_i <- train %>%
    group_by(movieId) %>%
    summarize(m_i = sum(rating - mu)/(n()+l))
  
  u_i <- train %>%
    left_join(m_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_i = sum(rating - m_i - mu)/(n()+l))
  
  predicted_ratings <- test %>%
    left_join(m_i, by = "movieId") %>% 
    left_join(u_i, by = "userId") %>%
    mutate(pred = mu + m_i + u_i) %>% mutate(pred = ifelse(pred < 0, 0, ifelse(pred > 5, 5, pred))) %>% .$pred
  
  return(RMSE(predicted_ratings, test$rating))
})
```

## **RMSE_Curve_Regularisation_Test:**  
```{r RMSE_curve_test, message = FALSE, warning = FALSE, echo = FALSE}
qplot(lambda_values, RMSE_function_reg,
      main = "Regularisation",
      xlab = "RMSE", ylab = "Lambda") # lambda vs RMSE
```

 
```{r optimum_lambda_test, message = FALSE, warning = FALSE, echo = FALSE}
lambda_opt <- lambda_values[which.min(RMSE_function_reg)]
res_rmse <- min(RMSE_function_reg)
# Lambda which minimizes RMSE
```  
Optimum $\lambda$ for Test: `r lambda_opt`  
RMSE for Test data: `r res_rmse`
\newpage

```{r table4, message = FALSE, warning = FALSE, echo = FALSE}
rmse_table <- rbind(rmse_table,
                    data_frame(Method = "User & Movie Effect Regularisation",
                               RMSE = min(RMSE_function_reg)))
rmse_table %>% knitr::kable(caption = "RMSEs")
```  

The *regularisation* gives us a higher RMSE than the first "User & Movie Effect" model. This is unexpected but since we have shrunk the population of the original data set, it is kind of expected.

## **Regularisation on *validation* data set:**  
It is time to see the effect of *regularisation* on the validation data set.  
```{r reg_val, message = FALSE, warning = FALSE}
RMSE_function_val_reg <- sapply(lambda_values, function(l){
  
  mu <- mean(edx$rating)
  
  m_i <- edx %>%
    group_by(movieId) %>%
    summarize(m_i = sum(rating - mu)/(n()+l))
  
  u_i <- edx %>%
    left_join(m_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_i = sum(rating - m_i - mu)/(n()+l))
  
  predicted_val_reg <- validation %>%
    left_join(m_i, by = "movieId") %>% 
    left_join(u_i, by = "userId") %>%
    mutate(pred = mu + m_i + u_i) %>% mutate(pred = ifelse(pred < 0, 0, ifelse(pred > 5, 5, pred))) %>% .$pred
  
  return(RMSE(predicted_val_reg, validation$rating, na.rm = T))
})
```

## **RMSE_Curve_Regularisation_Validation:**  
```{r RMSE_Curve_Val, message = FALSE, warning = FALSE, echo = FALSE}
qplot(lambda_values, RMSE_function_val_reg,
      main = "Regularisation on validation data set",
      xlab = "Lambda", ylab = "RMSE")
```


```{r Optimum_Lambda_Val, message = FALSE, warning = FALSE, echo = FALSE}
lambda_opt_reg <- lambda_values[which.min(RMSE_function_val_reg)]
# Lambda which minimizes RMSE
```
Optimum $\lambda$ for Validation: `r lambda_opt_reg`  


```{r RMSE_function_val_reg,  message = FALSE, warning = FALSE, echo = FALSE}
min_rmse <- min(RMSE_function_val_reg) # Best RMSE
```  
Final RMSE on Regularization: `r min_rmse`  
\newpage

# **Results:**  
```{r table_final, message = FALSE, warning = FALSE, echo = FALSE}
rmse_table <- rbind(rmse_table,
                    data_frame(Method = "User & Movie Effect Reg. on validation",
                               RMSE = min(RMSE_function_val_reg)))
rmse_table %>% knitr::kable(caption = "RMSEs")
```  


We can observe that better RMSE is obtained from the *User & Movie Effect* model. However, this RMSE is *only* obtained on the *test* set. Considering that we must trust more in the performance of the model when we predict from unseen data, we can say that the RMSE that results from the *User & Movie Effect with Regularisation on validation* (the last line in the table above) is our definitive model. This RMSE is obtained when $\lambda$=`r lambda_opt_reg` which has achieved **RMSE equal to** `r min_rmse`, successfully passing the target of 0.8649.  


# **Conclusion:**  

We started collecting and preparing the dataset for analysis, then we explored the information seeking for insights that might help during model build. Next, we created a random model that predicts the rating based on the probability distribution of each rating. This model gives the worst result.

We started the linear model with a very simple model which is just the mean of the observed ratings. From there, we added movie and user effects, that models the user behavior and movie distribution. With regularization we added a penalty value for the movies and users with few number of ratings. 


The variables `userId` and `movieId` have sufficient predictive power and could make better recommendations about movie to specific users of the streaming service. Therefore, the user could decide to spend more time using the service.

The RMSE equal to `r min_rmse` is pretty acceptable considering that we have few predictors, but both *User* and *Movie* effects are powerful enough to predict the `rating` that will be given to a movie, by a specific user.


# **Limitations and Future Work:**  
The model works only for existing users, movies and rating values, so the algorithm must run every time a new user or movie is included, or when the rating changes. This is not an issue for small client base and a few movies, but may become a concern for large data sets. The model should consider these changes and update the predictions as information changes.  

There is definitely a time delta effect on the average rating for a movie, based on either Actors associated to the movie or new movies that have similar genres and similar story lines that could inspire more users to rate older movies and hence can bring a significant change to the predicting model.  

Only two predictors are used, the movie and user information, not considering other features. Modern recommendation system models use many predictors, such as genres, bookmarks, play lists, history, etc.  

From a future perspective we can look at using Matrix Factorization with other ensemble methods to create a much more powerful predicting algorithm and develop a penalty based system for the whole algorithm to improve its performance over time.  
